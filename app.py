import streamlit as st
import feedparser
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# --- æŠ½å‡ºãƒ»è¦ç´„é–¢æ•°ã¯å‰å›ã¨åŒæ§˜ ---
def extract_text_from_url(url):
    try:
        response = requests.get(url, timeout=10)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "html.parser")
        for s in soup(["script", "style", "header", "footer", "nav", "aside"]):
            s.decompose()
        paragraphs = soup.find_all("p")
        text = "\n".join([p.get_text() for p in paragraphs])
        return text[:2500] 
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {e}"

def summarize_article(title, text, url):
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯SNSã§ã®æƒ…å ±ç™ºä¿¡ã«é•·ã‘ãŸåºƒå ±æ‹…å½“ã§ã™ã€‚"},
                {"role": "user", "content": f"""
ä»¥ä¸‹ã®è¨˜äº‹ã‚’è¦ç´„ã—ã€Xï¼ˆTwitterï¼‰å‘ã‘ã®æŠ•ç¨¿æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
æœ€å¾Œã«å¿…ãšã€Œå…ƒè¨˜äº‹ã®URLã€ã‚’æ·»ãˆã¦ãã ã•ã„ã€‚

ã€ãƒ«ãƒ¼ãƒ«ã€‘
1. ã€è¦‹å‡ºã—ã€‘ã‚’30æ–‡å­—ä»¥å†…ã§ä½œæˆã€‚
2. å†…å®¹ã‚’100æ–‡å­—ç¨‹åº¦ã§è¦ç´„ã€‚
3. æœ€å¾Œã«ã€Œè¨˜äº‹è©³ç´°ã¯ã“ã¡ã‚‰ï¼š{url}ã€ã¨ã€ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’2ã¤è¨˜è¼‰ã€‚

è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: {title}
æœ¬æ–‡: {text}
"""}
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}"

# --- UI éƒ¨åˆ† ---
st.set_page_config(page_title="News Summarizer", page_icon="ğŸ”—")
st.title("ğŸ“¡ RSS News Summarizer")

# ãƒ†ãƒ¼ãƒã”ã¨ã®ã‚½ãƒ¼ã‚¹è¨­å®šï¼ˆã“ã“ã‚’æœ€æ–°ãƒ»å°‚é–€ã‚½ãƒ¼ã‚¹ã«æ›´æ–°ï¼‰
THEMES = {
    "ç”ŸæˆAIæœ€æ–°æƒ…å ± (ITmedia AI+)": "https://www.itmedia.co.jp/news/subtop/ai/index.xml",
    "ç”ŸæˆAI/ãƒ†ãƒƒã‚¯ (ã‚®ã‚ºãƒ¢ãƒ¼ãƒ‰)": "https://www.gizmodo.jp/index.xml",
    "æš—å·è³‡ç”£/Web3 (CoinPost)": "https://coinpost.jp/?feed=rss2",
    "æœ€æ–°æŠ€è¡“å‹•å‘ (Publickey)": "https://www.publickey1.jp/atom.xml"
}

theme_choice = st.sidebar.radio("ãƒ†ãƒ¼ãƒã‚’é¸æŠ", list(THEMES.keys()))

if st.button(f"{theme_choice} ã®è¨˜äº‹ã‚’å–å¾—"):
    st.session_state['feed'] = feedparser.parse(THEMES[theme_choice])

if 'feed' in st.session_state:
    for entry in st.session_state['feed'].entries[:5]:
        with st.container():
            st.write(f"### {entry.title}")
            # --- ã“ã“ã§äº‹å‰ã«ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ãƒªãƒ³ã‚¯ã‚’è¨­ç½® ---
            st.markdown(f"[ğŸ”— å…ƒè¨˜äº‹ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§ç¢ºèªã™ã‚‹]({entry.link})")
            
            if st.button("ã“ã®å†…å®¹ã‚’è¦ç´„ã™ã‚‹", key=entry.link):
                st.session_state['selected_url'] = entry.link
                st.session_state['selected_title'] = entry.title
                
                with st.spinner("è¦ç´„ã‚’ç”Ÿæˆä¸­..."):
                    content = extract_text_from_url(entry.link)
                    summary = summarize_article(entry.title, content, entry.link)
                    st.session_state['final_summary'] = summary

# æœ€çµ‚çµæœã®è¡¨ç¤º
if 'final_summary' in st.session_state:
    st.divider()
    st.subheader("ğŸ“ ç”Ÿæˆã•ã‚ŒãŸæŠ•ç¨¿å†…å®¹ï¼ˆç¢ºèªç”¨ï¼‰")
    # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ãªã‚‰ä¿®æ­£ã‚‚å¯èƒ½
    edited_summary = st.text_area("å¿…è¦ã«å¿œã˜ã¦å¾®èª¿æ•´ã—ã¦ãã ã•ã„ï¼š", st.session_state['final_summary'], height=200)
    
    st.info(f"é€ä¿¡å…ˆURLç¢ºèª: {st.session_state['selected_url']}")
    
    if st.button("ğŸš€ Xã¸æŠ•ç¨¿ï¼ˆãƒ‡ãƒ¢ï¼‰"):
        st.success("æŠ•ç¨¿æ©Ÿèƒ½ã‚’é€£æºã™ã‚Œã°ã€ã“ã®å†…å®¹ãŒãã®ã¾ã¾é€ä¿¡ã•ã‚Œã¾ã™ï¼")